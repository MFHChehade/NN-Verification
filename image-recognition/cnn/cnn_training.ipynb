{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Optimal adversaries for convolutional MNIST model\n",
    "\n",
    "This notebook gives an example where OMLT is used to find adversarial examples for a trained convolutional neural network. We follow the below steps:<br>\n",
    "1.) A convolutional neural network (CNN) with ReLU activation functions is trained to classify images from the MNIST dataset <br>\n",
    "2.) OMLT is used to generate a mixed-integer encoding of the trained CNN using the big-M formulation <br>\n",
    "3.) The model is optimized to find the maximum classification error (defined by an \"adversarial\" label) over a small input region <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Setup\n",
    "This notebook assumes you have a working PyTorch environment to train the neural network for classification. The neural network is then formulated in Pyomo using OMLT which therefore requires working Pyomo and OMLT installations.\n",
    "\n",
    "The required Python libraries used this notebook are as follows: <br>\n",
    "- `numpy`: used for manipulate input data <br>\n",
    "- `torch`: the machine learning language we use to train our neural network\n",
    "- `torchvision`: a package containing the MNIST dataset\n",
    "- `pyomo`: the algebraic modeling language for Python, it is used to define the optimization model passed to the solver\n",
    "- `onnx`: used to express trained neural network models\n",
    "- `omlt`: the package this notebook demonstates. OMLT can formulate machine learning models (such as neural networks) within Pyomo\n",
    "\n",
    "**NOTE:** This notebook also assumes you have a working MIP solver executable (e.g., CBC, Gurobi) to solve optimization problems in Pyomo. The open-source solver CBC is called by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import requisite packages\n",
    "#data manipulation\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "#pytorch for training neural network\n",
    "import torch, torch.onnx\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "#pyomo for optimization\n",
    "import pyomo.environ as pyo\n",
    "\n",
    "#omlt for interfacing our neural network with pyomo\n",
    "from omlt import OmltBlock\n",
    "from omlt.neuralnet import FullSpaceNNFormulation\n",
    "from omlt.io.onnx import write_onnx_model_with_bounds, load_onnx_neural_network_with_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data and Train a Neural Network\n",
    "\n",
    "We begin by loading the MNIST dataset as `DataLoader` objects with pre-set training and testing batch sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set training and test batch sizes\n",
    "train_kwargs = {'batch_size': 64}\n",
    "test_kwargs = {'batch_size': 1000}\n",
    "\n",
    "#build DataLoaders for training and test sets\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transforms.ToTensor())\n",
    "dataset2 = datasets.MNIST('../data', train=False, transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the structure of the convolutional neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 10\n",
    "\n",
    "class Net(nn.Module):\n",
    "    #define layers of neural network\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1  = nn.Conv2d(1, 2, (4,4), (2,2), 0)\n",
    "        self.conv2  = nn.Conv2d(2, 2, (4,4), (2,2), 0)\n",
    "        self.hidden1 = nn.Linear(5*5*2, hidden_size)\n",
    "        self.output  = nn.Linear(hidden_size, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define forward pass of neural network\n",
    "    def forward(self, x):\n",
    "        self.x1 = self.conv1(x)\n",
    "        self.x2 = self.relu(self.x1)\n",
    "        self.x3 = self.conv2(self.x2)\n",
    "        self.x4 = self.relu(self.x3)\n",
    "        self.x5 = self.hidden1(self.x4.view((-1,5*5*2)))\n",
    "        self.x6 = self.relu(self.x5)\n",
    "        self.x7 = self.output(self.x6)\n",
    "        x = self.softmax(self.x7)      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define simple functions for training and testing the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training function computes loss and its gradient on batch, and prints status after every 200 batches\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train(); criterion = nn.NLLLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200  == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "#testing function computes loss and prints overall model accuracy on test set\n",
    "def test(model, test_loader):\n",
    "    model.eval(); criterion = nn.NLLLoss(reduction='sum')\n",
    "    test_loss = 0; correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  \n",
    "            pred = output.argmax(dim=1, keepdim=True) \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train the neural network on the dataset.\n",
    "Training here is performed using the `Adadelta` optimizer for five epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.292737\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.518931\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.740210\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.403113\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.711090\n",
      "\n",
      "Test set: Average loss: 0.3681, Accuracy: 8816/10000 (88%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.387400\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.284508\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.549140\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.238617\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.328351\n",
      "\n",
      "Test set: Average loss: 0.2623, Accuracy: 9197/10000 (92%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.103231\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.271127\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.539560\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.284458\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.139183\n",
      "\n",
      "Test set: Average loss: 0.2452, Accuracy: 9254/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.248313\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.169946\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.078817\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.145456\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.145222\n",
      "\n",
      "Test set: Average loss: 0.2205, Accuracy: 9341/10000 (93%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.386275\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.464936\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.357353\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.236445\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.254060\n",
      "\n",
      "Test set: Average loss: 0.2130, Accuracy: 9359/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define model and optimizer\n",
    "model = Net()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "#train CNN model for five epochs\n",
    "for epoch in range(5):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test(model, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pytorch model \n",
    "import torch\n",
    "torch.save(model.state_dict(), 'cnn_model.pth')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8f31a8284ce774e9ad8d309790c576c984c0620550967f9ef361ac8e66f487d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
